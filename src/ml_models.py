"""
ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ë¶„ì„ ëª¨ë“ˆ

ì—­í• : AI/ML ê¸°ë²•ì„ í™œìš©í•œ ë…¸ë™ì‹œì¥ ë°ì´í„° ë¶„ì„
- ì‹¤ì—…ë¥  ì˜ˆì¸¡ (Random Forest, XGBoost)
- ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ (K-Means)
- ì‹œê³„ì—´ ì˜ˆì¸¡ (Prophet)
- ìƒê´€ê´€ê³„ ë¶„ì„
"""

import logging
from pathlib import Path
from typing import Dict, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import mean_squared_error, r2_score, silhouette_score
from sqlalchemy.engine import Engine
from sqlalchemy import text

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).resolve().parents[1]
OUTPUT_DIR = PROJECT_ROOT / "output" / "ml_results"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# í•œê¸€ í°íŠ¸ ì„¤ì •
import platform
if platform.system() == "Darwin":  # macOS
    plt.rcParams["font.family"] = "AppleGothic"
else:
    plt.rcParams["font.family"] = ["NanumGothic", "Malgun Gothic", "sans-serif"]
plt.rcParams["axes.unicode_minus"] = False


def load_ml_dataset(engine: Engine) -> pd.DataFrame:
    """ML í•™ìŠµìš© í†µí•© ë°ì´í„°ì…‹ ìƒì„±"""

    query = text("""
    SELECT
        u.region_id,
        r.region_name,
        u.year_month,
        u.unemployment_rate,
        u.unemployment_level,
        u.labor_force,
        u.employed_persons,
        p.total_pop,
        -- íŒŒìƒ ë³€ìˆ˜
        CAST(u.labor_force AS FLOAT) / p.total_pop AS labor_force_ratio,
        CAST(u.employed_persons AS FLOAT) / p.total_pop AS employment_ratio,
        CAST(SUBSTR(u.year_month, 1, 4) AS INTEGER) AS year,
        CAST(SUBSTR(u.year_month, 6, 2) AS INTEGER) AS month
    FROM fact_unemployment_monthly u
    JOIN dim_region r ON u.region_id = r.region_id
    JOIN fact_population_monthly p
        ON u.region_id = p.region_id
        AND u.year_month = p.year_month
    WHERE u.unemployment_level IS NOT NULL
        AND u.labor_force IS NOT NULL
        AND p.total_pop IS NOT NULL
    ORDER BY u.year_month, u.region_id
    """)

    with engine.connect() as conn:
        df = pd.read_sql_query(query, conn)

    logger.info(f"âœ“ ML ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(df)}í–‰, {df.shape[1]}ê°œ ì»¬ëŸ¼")
    return df


def train_unemployment_predictor(df: pd.DataFrame) -> Dict:
    """ì‹¤ì—…ë¥  ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ (Random Forest + Gradient Boosting)

    ë…ë¦½ ë³€ìˆ˜ë§Œ ì‚¬ìš©í•˜ì—¬ ì‹¤ì—…ë¥  ì˜ˆì¸¡ (ìˆœí™˜ ë…¼ë¦¬ ì œê±°)
    - unemployment_level, labor_force, employed_persons ì œì™¸ (ì‹¤ì—…ë¥  ê³„ì‚°ì— ì§ì ‘ ì‚¬ìš©)
    - ì¸êµ¬, ì‹œê°„, ì§€ì—­ ë“± ì™¸ë¶€ ìš”ì¸ë§Œ ì‚¬ìš©
    """

    logger.info("=" * 80)
    logger.info("ğŸ¤– [AI ëª¨ë¸ 1] ì‹¤ì—…ë¥  ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    logger.info("=" * 80)

    # í”¼ì²˜ ì„ íƒ (ë…ë¦½ ë³€ìˆ˜ë§Œ ì‚¬ìš©)
    feature_cols = [
        "total_pop",           # ì´ ì¸êµ¬
        "labor_force_ratio",   # ê²½ì œí™œë™ì°¸ê°€ìœ¨
        "employment_ratio",    # ê³ ìš©ë¥ 
        "year",                # ì—°ë„ (ì‹œê°„ íŠ¸ë Œë“œ)
        "month",               # ì›” (ê³„ì ˆì„±)
        "region_id"            # ì§€ì—­ (ì¹´í…Œê³ ë¦¬)
    ]

    X = df[feature_cols].copy()
    y = df["unemployment_rate"]

    # ê²°ì¸¡ì¹˜ ì œê±°
    mask = X.notna().all(axis=1) & y.notna()
    X = X[mask]
    y = y[mask]

    # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ (ì‹œê°„ ìˆœì„œ ìœ ì§€)
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    logger.info(f"í•™ìŠµ ë°ì´í„°: {len(X_train)}ê±´, í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(X_test)}ê±´")

    # ëª¨ë¸ 1: Random Forest
    rf_model = RandomForestRegressor(
        n_estimators=200,
        max_depth=15,
        min_samples_split=5,
        random_state=42,
        n_jobs=-1
    )
    rf_model.fit(X_train, y_train)
    rf_pred = rf_model.predict(X_test)
    rf_r2 = r2_score(y_test, rf_pred)
    rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))

    # ëª¨ë¸ 2: Gradient Boosting
    gb_model = GradientBoostingRegressor(
        n_estimators=200,
        max_depth=5,
        learning_rate=0.1,
        random_state=42
    )
    gb_model.fit(X_train, y_train)
    gb_pred = gb_model.predict(X_test)
    gb_r2 = r2_score(y_test, gb_pred)
    gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))

    # êµì°¨ ê²€ì¦
    rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring="r2")
    gb_cv_scores = cross_val_score(gb_model, X_train, y_train, cv=5, scoring="r2")

    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“Š ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
    print("-" * 80)
    print(f"{'ëª¨ë¸':<20} {'RÂ² Score':<15} {'RMSE':<15} {'CV RÂ² (í‰ê· )':<15}")
    print("-" * 80)
    print(f"{'Random Forest':<20} {rf_r2:<15.4f} {rf_rmse:<15.4f} {rf_cv_scores.mean():<15.4f}")
    print(f"{'Gradient Boosting':<20} {gb_r2:<15.4f} {gb_rmse:<15.4f} {gb_cv_scores.mean():<15.4f}")
    print("-" * 80)

    # í”¼ì²˜ ì¤‘ìš”ë„
    feature_importance = pd.DataFrame({
        "feature": feature_cols,
        "importance": rf_model.feature_importances_
    }).sort_values("importance", ascending=False)

    print("\nğŸ” í”¼ì²˜ ì¤‘ìš”ë„ (Random Forest)")
    print("-" * 80)
    for idx, row in feature_importance.head(5).iterrows():
        print(f"{row['feature']:<30} {row['importance']:.4f}")

    # ì‹œê°í™”: ì˜ˆì¸¡ vs ì‹¤ì œ
    fig, axes = plt.subplots(1, 2, figsize=(14, 5))

    # Random Forest
    axes[0].scatter(y_test, rf_pred, alpha=0.5, s=10)
    axes[0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    axes[0].set_xlabel("ì‹¤ì œ ì‹¤ì—…ë¥  (%)")
    axes[0].set_ylabel("ì˜ˆì¸¡ ì‹¤ì—…ë¥  (%)")
    axes[0].set_title(f"Random Forest (RÂ²={rf_r2:.4f})")
    axes[0].grid(True, alpha=0.3)

    # Gradient Boosting
    axes[1].scatter(y_test, gb_pred, alpha=0.5, s=10, color='green')
    axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
    axes[1].set_xlabel("ì‹¤ì œ ì‹¤ì—…ë¥  (%)")
    axes[1].set_ylabel("ì˜ˆì¸¡ ì‹¤ì—…ë¥  (%)")
    axes[1].set_title(f"Gradient Boosting (RÂ²={gb_r2:.4f})")
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "01_unemployment_prediction.png", dpi=300, bbox_inches="tight")
    logger.info(f"âœ“ ê·¸ë˜í”„ ì €ì¥: {OUTPUT_DIR / '01_unemployment_prediction.png'}")
    plt.close()

    # í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”
    fig, ax = plt.subplots(figsize=(10, 6))
    feature_importance_top = feature_importance.head(8)
    ax.barh(feature_importance_top["feature"], feature_importance_top["importance"])
    ax.set_xlabel("ì¤‘ìš”ë„")
    ax.set_title("ì‹¤ì—…ë¥  ì˜ˆì¸¡ í”¼ì²˜ ì¤‘ìš”ë„ (Random Forest)")
    ax.invert_yaxis()
    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "02_feature_importance.png", dpi=300, bbox_inches="tight")
    logger.info(f"âœ“ ê·¸ë˜í”„ ì €ì¥: {OUTPUT_DIR / '02_feature_importance.png'}")
    plt.close()

    return {
        "rf_model": rf_model,
        "gb_model": gb_model,
        "rf_r2": rf_r2,
        "gb_r2": gb_r2,
        "rf_rmse": rf_rmse,
        "gb_rmse": gb_rmse,
        "feature_importance": feature_importance
    }


def cluster_regions(df: pd.DataFrame) -> Dict:
    """ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ (K-Means)"""

    logger.info("\n" + "=" * 80)
    logger.info("ğŸ¤– [AI ëª¨ë¸ 2] ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ (K-Means)")
    logger.info("=" * 80)

    # ì§€ì—­ë³„ í‰ê·  í†µê³„ ê³„ì‚°
    region_stats = df.groupby("region_name").agg({
        "unemployment_rate": ["mean", "std"],
        "labor_force_ratio": "mean",
        "employment_ratio": "mean",
        "labor_force": "mean"
    }).reset_index()

    region_stats.columns = [
        "region_name", "avg_unemployment_rate", "std_unemployment_rate",
        "avg_labor_force_ratio", "avg_employment_ratio", "avg_labor_force"
    ]

    # ê²°ì¸¡ì¹˜ ì œê±°
    region_stats = region_stats.dropna()

    # í”¼ì²˜ ì„ íƒ ë° ì •ê·œí™” (ìƒê´€ê´€ê³„ ë†’ì€ ë³€ìˆ˜ ì œê±°)
    # labor_force_ratioì™€ employment_ratioëŠ” ìƒê´€ë„ê°€ ë§¤ìš° ë†’ìœ¼ë¯€ë¡œ í•˜ë‚˜ë§Œ ì‚¬ìš©
    feature_cols = [
        "avg_unemployment_rate",
        "std_unemployment_rate",
        "avg_employment_ratio"  # labor_force_ratio ì œê±° (ì¤‘ë³µì„±)
    ]
    X = region_stats[feature_cols]

    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    # ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ì°¾ê¸° (Elbow Method + Silhouette)
    inertias = []
    silhouette_scores = []
    K_range = range(2, 8)

    for k in K_range:
        kmeans = KMeans(n_clusters=k, random_state=42, n_init=20, max_iter=500)  # n_init ì¦ê°€
        kmeans.fit(X_scaled)
        inertias.append(kmeans.inertia_)
        silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

    # ìµœì  K ìë™ ì„ íƒ (Silhouette Scoreê°€ ê°€ì¥ ë†’ì€ K)
    optimal_k = K_range[silhouette_scores.index(max(silhouette_scores))]
    print(f"\nğŸ” ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ íƒìƒ‰:")
    for k, score in zip(K_range, silhouette_scores):
        marker = " â­ ìµœì " if k == optimal_k else ""
        print(f"   K={k}: Silhouette Score = {score:.3f}{marker}")
    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
    region_stats["cluster"] = kmeans.fit_predict(X_scaled)

    # í’ˆì§ˆ ì§€í‘œ ê³„ì‚°
    silhouette_avg = silhouette_score(X_scaled, kmeans.labels_)

    print(f"\nğŸ“Š í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ (K={optimal_k})")
    print("-" * 80)
    print(f"âœ“ Silhouette Score: {silhouette_avg:.3f} (í’ˆì§ˆ ì§€í‘œ: -1~1, ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)")
    print(f"âœ“ Inertia: {kmeans.inertia_:.2f} (í´ëŸ¬ìŠ¤í„° ë‚´ë¶€ ê±°ë¦¬ í•©)")
    print("-" * 80)

    for cluster_id in range(optimal_k):
        cluster_regions = region_stats[region_stats["cluster"] == cluster_id]
        print(f"\nğŸ”¹ í´ëŸ¬ìŠ¤í„° {cluster_id + 1} ({len(cluster_regions)}ê°œ ì§€ì—­)")
        print(f"   ì§€ì—­: {', '.join(cluster_regions['region_name'].tolist())}")
        print(f"   í‰ê·  ì‹¤ì—…ë¥ : {cluster_regions['avg_unemployment_rate'].mean():.2f}%")
        print(f"   ì‹¤ì—…ë¥  ë³€ë™ì„±: {cluster_regions['std_unemployment_rate'].mean():.2f}")

    # ì‹œê°í™”: í´ëŸ¬ìŠ¤í„°ë³„ ë¶„í¬
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))

    # Elbow Method
    axes[0].plot(K_range, inertias, marker='o', linewidth=2)
    axes[0].set_xlabel("í´ëŸ¬ìŠ¤í„° ìˆ˜ (K)")
    axes[0].set_ylabel("Inertia")
    axes[0].set_title("Elbow Method")
    axes[0].grid(True, alpha=0.3)
    axes[0].axvline(x=3, color='red', linestyle='--', alpha=0.5)

    # Silhouette Score
    axes[1].plot(K_range, silhouette_scores, marker='o', linewidth=2, color='green')
    axes[1].set_xlabel("í´ëŸ¬ìŠ¤í„° ìˆ˜ (K)")
    axes[1].set_ylabel("Silhouette Score")
    axes[1].set_title("Silhouette Analysis")
    axes[1].grid(True, alpha=0.3)
    axes[1].axvline(x=optimal_k, color='red', linestyle='--', alpha=0.5)
    axes[1].axhline(y=0.5, color='orange', linestyle=':', alpha=0.5, label='Good threshold')
    axes[1].legend()

    # í´ëŸ¬ìŠ¤í„° ì‹œê°í™” (2D: ì‹¤ì—…ë¥  vs ë³€ë™ì„±)
    colors = plt.cm.Set1(range(optimal_k))  # ë™ì ìœ¼ë¡œ ìƒ‰ìƒ ìƒì„±
    for cluster_id in range(optimal_k):
        cluster_data = region_stats[region_stats["cluster"] == cluster_id]
        axes[2].scatter(
            cluster_data["avg_unemployment_rate"],
            cluster_data["std_unemployment_rate"],
            c=colors[cluster_id],
            label=f"í´ëŸ¬ìŠ¤í„° {cluster_id + 1}",
            s=100,
            alpha=0.6
        )

        # ì§€ì—­ëª… í‘œì‹œ
        for idx, row in cluster_data.iterrows():
            axes[2].annotate(
                row["region_name"],
                (row["avg_unemployment_rate"], row["std_unemployment_rate"]),
                fontsize=8,
                alpha=0.7
            )

    axes[2].set_xlabel("í‰ê·  ì‹¤ì—…ë¥  (%)")
    axes[2].set_ylabel("ì‹¤ì—…ë¥  í‘œì¤€í¸ì°¨")
    axes[2].set_title(f"ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§ (K={optimal_k})")
    axes[2].legend()
    axes[2].grid(True, alpha=0.3)

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "03_region_clustering.png", dpi=300, bbox_inches="tight")
    logger.info(f"âœ“ ê·¸ë˜í”„ ì €ì¥: {OUTPUT_DIR / '03_region_clustering.png'}")
    plt.close()

    return {
        "kmeans": kmeans,
        "region_stats": region_stats,
        "optimal_k": optimal_k,
        "silhouette_score": silhouette_score(X_scaled, kmeans.labels_)
    }


def time_series_trend_analysis(df: pd.DataFrame) -> Dict:
    """ì‹œê³„ì—´ ê¸°ìˆ í†µê³„ - ì—°ë„ë³„ ì‹¤ì—…ë¥  ë³€í™”"""

    logger.info("\n" + "=" * 80)
    logger.info("ğŸ“Š [ê¸°ìˆ í†µê³„] ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„")
    logger.info("=" * 80)

    # ì—°ë„ë³„, ì§€ì—­ë³„ í‰ê·  ì‹¤ì—…ë¥ 
    yearly_trend = df.groupby(["year", "region_name"])["unemployment_rate"].mean().reset_index()

    # ì „ì²´ í‰ê·  íŠ¸ë Œë“œ
    overall_trend = df.groupby("year")["unemployment_rate"].agg(["mean", "std"]).reset_index()

    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 1, figsize=(14, 10))

    # 1. ì§€ì—­ë³„ ì‹œê³„ì—´
    for region in df["region_name"].unique():
        region_data = yearly_trend[yearly_trend["region_name"] == region]
        axes[0].plot(region_data["year"], region_data["unemployment_rate"],
                    marker='o', label=region, alpha=0.7, linewidth=2)

    axes[0].set_xlabel("ì—°ë„")
    axes[0].set_ylabel("ì‹¤ì—…ë¥  (%)")
    axes[0].set_title("ì§€ì—­ë³„ ì‹¤ì—…ë¥  ì¶”ì´ (2017-2024)")
    axes[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)
    axes[0].grid(True, alpha=0.3)
    axes[0].axvline(x=2020, color='red', linestyle='--', alpha=0.5, label='COVID-19')

    # 2. ì „ì²´ í‰ê·  + í‘œì¤€í¸ì°¨
    axes[1].plot(overall_trend["year"], overall_trend["mean"],
                marker='o', linewidth=3, color='blue', label='ì „êµ­ í‰ê· ')
    axes[1].fill_between(overall_trend["year"],
                         overall_trend["mean"] - overall_trend["std"],
                         overall_trend["mean"] + overall_trend["std"],
                         alpha=0.3, color='blue', label='í‘œì¤€í¸ì°¨ ë²”ìœ„')
    axes[1].set_xlabel("ì—°ë„")
    axes[1].set_ylabel("ì‹¤ì—…ë¥  (%)")
    axes[1].set_title("ì „êµ­ í‰ê·  ì‹¤ì—…ë¥  ì¶”ì´ (í‘œì¤€í¸ì°¨ í¬í•¨)")
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)
    axes[1].axvline(x=2020, color='red', linestyle='--', alpha=0.5, label='COVID-19')

    plt.tight_layout()
    plt.savefig(OUTPUT_DIR / "04_time_series_trend.png", dpi=300, bbox_inches="tight")
    logger.info(f"âœ“ ê·¸ë˜í”„ ì €ì¥: {OUTPUT_DIR / '04_time_series_trend.png'}")
    plt.close()

    # í†µê³„ ì¶œë ¥
    print("\nğŸ“Š ì—°ë„ë³„ ì „êµ­ í‰ê·  ì‹¤ì—…ë¥ ")
    print("-" * 80)
    print(f"{'ì—°ë„':<10} {'í‰ê·  ì‹¤ì—…ë¥ ':<15} {'í‘œì¤€í¸ì°¨':<15} {'ìµœì†Œ':<10} {'ìµœëŒ€':<10}")
    print("-" * 80)

    for _, row in overall_trend.iterrows():
        year_data = df[df["year"] == row["year"]]["unemployment_rate"]
        print(f"{int(row['year']):<10} {row['mean']:<15.2f} {row['std']:<15.2f} "
              f"{year_data.min():<10.2f} {year_data.max():<10.2f}")

    # COVID-19 ì „í›„ ë¹„êµ
    pre_covid = overall_trend[overall_trend["year"] < 2020]["mean"].mean()
    post_covid = overall_trend[overall_trend["year"] >= 2020]["mean"].mean()

    print(f"\nğŸ’¡ ì£¼ìš” ì¸ì‚¬ì´íŠ¸:")
    print(f"   - COVID-19 ì´ì „ í‰ê· : {pre_covid:.2f}%")
    print(f"   - COVID-19 ì´í›„ í‰ê· : {post_covid:.2f}%")
    print(f"   - ë³€í™”í­: {post_covid - pre_covid:+.2f}%p")

    if post_covid > pre_covid:
        print(f"   âš ï¸  COVID-19 ì´í›„ ì‹¤ì—…ë¥ ì´ {post_covid - pre_covid:.2f}%p ìƒìŠ¹")
    else:
        print(f"   âœ… COVID-19 ì´í›„ì—ë„ ì‹¤ì—…ë¥  ê°ì†Œì„¸ ìœ ì§€")

    return {
        "yearly_trend": yearly_trend,
        "overall_trend": overall_trend,
        "pre_covid_avg": pre_covid,
        "post_covid_avg": post_covid
    }


def run_all_ml_models(engine: Engine) -> Dict:
    """ëª¨ë“  ML ëª¨ë¸ ì‹¤í–‰"""

    logger.info("\n" + "=" * 80)
    logger.info("ğŸš€ AI/ML ë¶„ì„ ì‹œì‘")
    logger.info("=" * 80)

    # ë°ì´í„° ë¡œë“œ
    df = load_ml_dataset(engine)

    results = {}

    # 1. ì‹¤ì—…ë¥  ì˜ˆì¸¡ ëª¨ë¸
    results["prediction"] = train_unemployment_predictor(df)

    # 2. ì§€ì—­ í´ëŸ¬ìŠ¤í„°ë§
    results["clustering"] = cluster_regions(df)

    # 3. ì‹œê³„ì—´ íŠ¸ë Œë“œ ë¶„ì„
    results["time_series"] = time_series_trend_analysis(df)

    logger.info("\n" + "=" * 80)
    logger.info("âœ… AI/ML ë¶„ì„ ì™„ë£Œ!")
    logger.info(f"ğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {OUTPUT_DIR}")
    logger.info("=" * 80)

    return results


if __name__ == "__main__":
    from db_loader import DBConfig

    config = DBConfig()
    engine = config.make_engine()

    results = run_all_ml_models(engine)
